{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "CNN_RNNCustomEmbdedding.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdwA_jnVx-7M",
        "colab_type": "text"
      },
      "source": [
        "##Creating a custom word embedding to understand sentiment of tweets with regular english words and emojis. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUfbARI5OUkq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "eb568771-4d65-42bb-879f-c0236e6c2c83"
      },
      "source": [
        "#Install GetOldTweets (A package that allows us to query twitter and pull old tweets)\n",
        "!pip install GetOldTweets3"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting GetOldTweets3\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/f4/a00c2a7c90801abc875325bb5416ce9090ac86d06a00cc887131bd73ba45/GetOldTweets3-0.0.11-py3-none-any.whl\n",
            "Collecting pyquery>=1.2.10\n",
            "  Downloading https://files.pythonhosted.org/packages/78/43/95d42e386c61cb639d1a0b94f0c0b9f0b7d6b981ad3c043a836c8b5bc68b/pyquery-1.4.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from GetOldTweets3) (4.2.6)\n",
            "Collecting cssselect>0.7.9\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Installing collected packages: cssselect, pyquery, GetOldTweets3\n",
            "Successfully installed GetOldTweets3-0.0.11 cssselect-1.1.0 pyquery-1.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcQOefDyOvdh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "60abdc56-4063-4546-fe96-29c394304c29"
      },
      "source": [
        "#Mounting drive to get the word embedding and vector embedding bins\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5uhbDNjOUkw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import necessary packages\n",
        "import gensim.models as gs\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dBbtrnUyQJ9",
        "colab_type": "text"
      },
      "source": [
        "#Creating the word embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Rl_a42FOUkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating a Phrase2vec class to handle the concatenated dict (taken from github)\n",
        "class Phrase2Vec:\n",
        "    \"\"\"Wrapper for the word2vec model and emoji2vec model, allowing us to compute phrases\"\"\"\n",
        "    def __init__(self, dim, w2v, e2v=None):\n",
        "        \"\"\"Constructor for the Phrase2Vec model\n",
        "        Args:\n",
        "            dim: Dimension of the vectors in word2vec and emoji2vec\n",
        "            w2v: Gensim object for word2vec\n",
        "            e2v: Gensim object for emoji2vec\n",
        "        \"\"\"\n",
        "        self.wordVecModel = w2v\n",
        "        if e2v is not None:\n",
        "            self.emojiVecModel = e2v\n",
        "        else:\n",
        "            self.emojiVecModel = dict()\n",
        "        self.dimension = dim\n",
        "\n",
        "    @classmethod\n",
        "    def from_word2vec_paths(cls, dim, w2v_path='/data/word2vec/GoogleNews-vectors-negative300.bin',\n",
        "                            e2v_path=None):\n",
        "        \"\"\"Creates a Phrase2Vec object based on paths for w2v and e2v\n",
        "        Args:\n",
        "            dim: Dimension of the vectors in word2vec and emoji2vec\n",
        "            w2v_path: Path to word2vec vectors\n",
        "            e2v_path: Path to emoji2vec vectors\n",
        "        Returns:\n",
        "        \"\"\"\n",
        "        if not os.path.exists(w2v_path):\n",
        "            print(str.format('{} not found. Either provide a different path, or download binary from '\n",
        "                             'https://code.google.com/archive/p/word2vec/ and unzip', w2v_path))\n",
        "\n",
        "        w2v = gs.Word2Vec.load_word2vec_format(w2v_path, binary=True)\n",
        "        if e2v_path is not None:\n",
        "            e2v = gs.Word2Vec.load_word2vec_format(e2v_path, binary=True)\n",
        "        else:\n",
        "            e2v = dict()\n",
        "        return cls(dim, w2v, e2v)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        \"\"\"Get the vector sum of all tokens in a phrase\n",
        "        Args:\n",
        "            item: Phrase to be converted into a vector sum\n",
        "        Returns:\n",
        "            phr_sum: Bag-of-words sum of the tokens in the phrase supplied\n",
        "        \"\"\"\n",
        "        tokens = item.split(' ')\n",
        "        phr_sum = np.zeros(self.dimension, np.float32)\n",
        "\n",
        "        for token in tokens:\n",
        "            if token in self.wordVecModel:\n",
        "                phr_sum += self.wordVecModel[token]\n",
        "            elif token in self.emojiVecModel:\n",
        "                phr_sum += self.emojiVecModel[token]\n",
        "\n",
        "        return phr_sum\n",
        "\n",
        "    def from_emoji(self, emoji_vec, top_n=10):\n",
        "        \"\"\"Get the top n closest tokens for a supplied emoji vector\n",
        "        Args:\n",
        "            emoji_vec: Emoji vector\n",
        "            top_n: number of results to return\n",
        "        Returns:\n",
        "            Closest n tokens for a supplied emoji_vec\n",
        "        \"\"\"\n",
        "        return self.wordVecModel.most_similar(positive=emoji_vec, negative=[], topn=top_n)\n",
        "\n",
        "    def __setitem__(self, key, value):\n",
        "        self.wordVecModel[key] = value"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25N4cgl2K__6",
        "colab_type": "text"
      },
      "source": [
        "The emoji2vec bin was taken from https://github.com/uclnlp/emoji2vec\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2h3hubhOUk2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "c3f915a3-46cb-40a3-d7e9-b337a257b387"
      },
      "source": [
        "#Read in the emoji vect word embedding bin\n",
        "e2v_path = \"/content/drive/My Drive/emoji2vec.bin\"\n",
        "e2v = gs.KeyedVectors.load_word2vec_format(e2v_path, binary=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHfwt2dzZA4s",
        "colab_type": "text"
      },
      "source": [
        "The word2vec bin was taken from https://github.com/mmihaltz/word2vec-GoogleNews-vectors\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el6eCrPoOUk5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "c9bde2b1-adaf-4327-a383-c072f54556aa"
      },
      "source": [
        "#Read in the actual word vector embedding bin for words\n",
        "w2v_path = \"/content/drive/My Drive/GoogleNews-vectors-negative300.bin\"\n",
        "w2v = gs.KeyedVectors.load_word2vec_format(w2v_path, binary=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Izps-wyJPBSW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# !gunzip '/content/drive/My Drive/GoogleNews-vectors-negative300.bin'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxTPDod0OUk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out_dim = 300\n",
        "#Gives dictionary so when you call the word/emoji, you'll get a reference vector\n",
        "p2v_our_emoji = Phrase2Vec(out_dim, w2v, e2v=e2v)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAcGtH40OUlA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tokenize the new concatenated dictionary\n",
        "p2vemojiToken = list(e2v.vocab.keys()) + list(w2v.vocab.keys())\n",
        "\n",
        "word2index = dict((v,k) for (k,v) in enumerate(p2vemojiToken))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp9OQDfuOUlD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create the embedding matrix. len+1 to add an unknown bin for all unknown tokens. \n",
        "embedding_matrix = np.zeros((len(p2vemojiToken) + 1, 300)).astype(np.float32)\n",
        "for i, word in enumerate(p2vemojiToken):\n",
        "    embedding_vector = p2v_our_emoji[word]\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpfDYYXYOUlG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ca858a0a-b282-4734-f1d9-0dbba690b767"
      },
      "source": [
        "#Clean up resources that are noppt being used\n",
        "del p2v_our_emoji,w2v,e2v\n",
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "207"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnGq-hlizPk1",
        "colab_type": "text"
      },
      "source": [
        "##Read in and prepare the training set. The training set is taken from the \"Sentiment140\" Set. It is basically a labeled set of positive, negative, and neutral tweets. In total, the set consists of 1.6 million tweets. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ct6vBhXMOUlT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Set unknown bin\n",
        "unknown = len(embedding_matrix)-1\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "\n",
        "#read in 1.6 million tweets that were prelabeled. The set is known as the Sentiment140 set. \n",
        "trialdata = pd.read_csv('/content/drive/My Drive/training.1600000.processed.noemoticon.csv', nrows = 16000000, encoding = 'latin-1')\n",
        "trialdata.columns = ['sentiment','id','date','device','user','tweet']\n",
        "\n",
        "#create xtrail and y trial \n",
        "xtrial,ytrial = unknown*np.ones((len(trialdata),MAX_SEQUENCE_LENGTH)).astype(np.int),[]\n",
        "#The training set has the labeled tweets as 0 for neg, 2 for neutral, and 4 for positive\n",
        "sentiments = [0,4,2]\n",
        "\n",
        "\n",
        "#Iterate through tweets and add to xtrial/ytrial\n",
        "for i,row in trialdata.iterrows():\n",
        "    #print(row['text'])\n",
        "    tweet = row['tweet'].split()\n",
        "    xtrial[i,:len(tweet)] = [word2index.get(word,unknown) for word in row['tweet'].split()][:MAX_SEQUENCE_LENGTH]\n",
        "    ytrial.append(sentiments.index(row['sentiment']))\n",
        "\n",
        "ytrial = np.eye(3)[np.array(ytrial)]\n",
        "\n",
        "#Split to train and validation set. \n",
        "index = np.arange(len(xtrial))\n",
        "np.random.shuffle(index)\n",
        "xtrain = xtrial[index[2000:]]\n",
        "ytrain = ytrial[index[2000:]]\n",
        "\n",
        "\n",
        "xvalid = xtrial[index[:2000]]\n",
        "yvalid = ytrial[index[:2000]]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhgY41VPOUlW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f097a6ca-171d-4b53-f92a-9301e1effaee"
      },
      "source": [
        "print(xtrain.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1597999, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH6fzfqLOUll",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "64cae2fb-06ac-4094-dee2-1d1e888dac65"
      },
      "source": [
        "import keras \n",
        "from keras.layers import Embedding,Input,Dense,MaxPooling1D\n",
        "\n",
        "#Create the Keras embedding layer\n",
        "embedding_layer = Embedding(len(p2vemojiToken) + 1,\n",
        "                            300,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length= MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=False)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "5MyT8Al1OUlo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "d8da98c9-978c-4ff8-d7d0-4aacc636806c"
      },
      "source": [
        "from keras import Model\n",
        "from keras.layers import Embedding,Input,Dense,MaxPooling1D, Conv1D, Reshape\n",
        "\n",
        "stride = 1\n",
        "filtersize = 10\n",
        "\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "x = Conv1D(128, filtersize, activation='relu',strides =stride)(embedded_sequences)\n",
        "x = MaxPooling1D(((MAX_SEQUENCE_LENGTH-filtersize)/stride,))(x)\n",
        "x = Reshape((128,))(x)\n",
        "preds = Dense(3, activation='softmax')(x)\n",
        "\n",
        "model = Model(sequence_input, preds)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 50, 300)           900498600 \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 41, 128)           384128    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 1.0, 128)          0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 387       \n",
            "=================================================================\n",
            "Total params: 900,883,115\n",
            "Trainable params: 384,515\n",
            "Non-trainable params: 900,498,600\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4LjFFxXOUlt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "430ef4e1-ae27-4915-a2bc-84c1637d5514"
      },
      "source": [
        "# happy learning!\n",
        "model.fit(xtrain, ytrain, epochs=3,batch_size=128, validation_data = (xvalid,yvalid))\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1597999 samples, validate on 2000 samples\n",
            "Epoch 1/3\n",
            "1597999/1597999 [==============================] - 60s 37us/step - loss: 0.4672 - acc: 0.7773 - val_loss: 0.4711 - val_acc: 0.7735\n",
            "Epoch 2/3\n",
            "1597999/1597999 [==============================] - 52s 33us/step - loss: 0.4295 - acc: 0.8010 - val_loss: 0.4561 - val_acc: 0.7830\n",
            "Epoch 3/3\n",
            "1597999/1597999 [==============================] - 53s 33us/step - loss: 0.4073 - acc: 0.8142 - val_loss: 0.4552 - val_acc: 0.7915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f64f2d152b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXSjng_IOUlv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5d7cefc3-91a2-4da8-a2e2-2174ef043203"
      },
      "source": [
        "#testprobs = model.predict(xtrial)\n",
        "\n",
        "#testpreds = np.argmax(testprobs, axis=1)\n",
        "#np.array(sentiments)[testpreds]\n",
        "\n",
        "model.evaluate(xvalid,yvalid)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000/2000 [==============================] - 0s 61us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.45520978164672854, 0.7914999723434448]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnNOYqYXxS31",
        "colab_type": "text"
      },
      "source": [
        "#RNN Model with the same word embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jQnKfkeOUme",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "a28ff61b-36ea-4fa0-a239-f82cabe40d83"
      },
      "source": [
        "#RNN Model\n",
        "from keras.layers import LSTM\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "filtersize = 10\n",
        "opt = RMSprop(learning_rate = 0.0003)\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "x = LSTM(64)(embedded_sequences)\n",
        "\n",
        "preds = Dense(3, activation='softmax')(x)\n",
        "\n",
        "\n",
        "model1 = Model(sequence_input, preds)\n",
        "model1.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['acc'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 50, 300)           900498600 \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 41, 128)           384128    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 1.0, 128)          0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 387       \n",
            "=================================================================\n",
            "Total params: 900,883,115\n",
            "Trainable params: 384,515\n",
            "Non-trainable params: 900,498,600\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnbCXYtrOUmi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "2e9c4a7e-476b-4c5a-bfc8-1cd63bf64545"
      },
      "source": [
        "# happy learning!\n",
        "model1.fit(xtrain, ytrain, epochs=3,batch_size=128, validation_data = (xvalid,yvalid))\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1597999 samples, validate on 2000 samples\n",
            "Epoch 1/3\n",
            "1597999/1597999 [==============================] - 874s 547us/step - loss: 0.5053 - acc: 0.7517 - val_loss: 0.4975 - val_acc: 0.7605\n",
            "Epoch 2/3\n",
            "1597999/1597999 [==============================] - 848s 531us/step - loss: 0.4590 - acc: 0.7821 - val_loss: 0.4796 - val_acc: 0.7620\n",
            "Epoch 3/3\n",
            "1597999/1597999 [==============================] - 848s 531us/step - loss: 0.4440 - acc: 0.7913 - val_loss: 0.4630 - val_acc: 0.7730\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f64f2e45ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8s-JTVDfOUmm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "dc777434-156c-43f0-ff60-22da35c064b0"
      },
      "source": [
        "model1.evaluate(xtrain,ytrain)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1597999/1597999 [==============================] - 393s 246us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.44766011127058875, 0.7852995991706848]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiX11xyDxZAP",
        "colab_type": "text"
      },
      "source": [
        "#CNN Model with an RNN Layer with the same word embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NW6RFw8FOUmx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "75c8920f-692d-414f-e8d1-5f42f7bcb03b"
      },
      "source": [
        "#Stacking models (using RNN as a layer in CNN)\n",
        "filtersize = 5\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "opt = RMSprop(learning_rate = 0.0003)\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "x = Conv1D(32, filtersize, activation='relu', strides = 2)(embedded_sequences)\n",
        "x = LSTM(32)(x)\n",
        "x = Reshape((32,))(x)\n",
        "# x = Conv1D(128, 5, activation='relu')(x)\n",
        "# x = MaxPooling1D(5)(x)\n",
        "# x = Conv1D(128, 5, activation='relu')(x)\n",
        "# x = MaxPooling1D(35)(x)  # global max pooling\n",
        "# x = Flatten()(x)\n",
        "# x = Dense(128, activation='relu')(x)\n",
        "preds = Dense(3, activation='softmax')(x)\n",
        "#preds = Dense(len(labels_index), activation='softmax')(x)\n",
        "\n",
        "model2 = Model(sequence_input, preds)\n",
        "model2.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['acc'])\n",
        "\n",
        "print(model2.summary())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 50, 300)           900498600 \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 23, 32)            48032     \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3)                 99        \n",
            "=================================================================\n",
            "Total params: 900,555,051\n",
            "Trainable params: 56,451\n",
            "Non-trainable params: 900,498,600\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY84d__dOUmz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "6c41deef-230a-4c7e-bc9d-9f298ad1388c"
      },
      "source": [
        "# happy learning!\n",
        "model2.fit(xtrain, ytrain,\n",
        "          epochs=3,batch_size=128, validation_data = (xvalid,yvalid))\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1597999 samples, validate on 2000 samples\n",
            "Epoch 1/3\n",
            "1597999/1597999 [==============================] - 442s 277us/step - loss: 0.5067 - acc: 0.7484 - val_loss: 0.4895 - val_acc: 0.7570\n",
            "Epoch 2/3\n",
            "1597999/1597999 [==============================] - 429s 268us/step - loss: 0.4626 - acc: 0.7790 - val_loss: 0.4687 - val_acc: 0.7835\n",
            "Epoch 3/3\n",
            "1597999/1597999 [==============================] - 427s 267us/step - loss: 0.4485 - acc: 0.7880 - val_loss: 0.4563 - val_acc: 0.7885\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f64f310ec88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Na_b2rg8OUm4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c1022607-c667-4ee1-d514-63da105f74d4"
      },
      "source": [
        "model.evaluate(xtrial,ytrial)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1599999/1599999 [==============================] - 78s 49us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.35055361391133444, 0.8426399230957031]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nQxSVCoZ4Kc",
        "colab_type": "text"
      },
      "source": [
        "Hand labeled set evaluated using the best model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgIExmAU88v_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "eb9a888b-6bb2-4c6e-f702-d02541751c25"
      },
      "source": [
        "emojidata = pd.read_csv('/content/LabeledTweets.csv')\n",
        "print(emojidata.head())\n",
        "\n",
        "#create xtrail and y trial \n",
        "xemoji,yemoji = unknown*np.ones((len(emojidata),MAX_SEQUENCE_LENGTH)).astype(np.int),[]\n",
        "#The training set has the labeled tweets as 0 for neg, 2 for neutral, and 4 for positive\n",
        "sentiments = [0,4,1]\n",
        "\n",
        "\n",
        "#Iterate through tweets and add to xtrial/ytrial\n",
        "for i,row in emojidata.iterrows():\n",
        "    #print(row['text'])\n",
        "    tweet = row['Tweet'].split()\n",
        "    xemoji[i,:len(tweet)] = [word2index.get(word,unknown) for word in row['Tweet'].split()][:MAX_SEQUENCE_LENGTH]\n",
        "    yemoji.append(sentiments.index(row['Label']))\n",
        "print(yemoji)\n",
        "for n,i in enumerate(yemoji):\n",
        "  if i == 2:\n",
        "    yemoji[n] = 4\n",
        "\n",
        "# yemoji = np.eye(3)[np.array(yemoji)]\n",
        "  \n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Label                                              Tweet\n",
            "0      1  The original manuscript of the #UN Charter pre...\n",
            "1      0  Where is #BML on this?  How is this ok, partic...\n",
            "2      0  @pritipatel Presumeably Cressida Dick will obt...\n",
            "3      1  God speed to the person filming this and the a...\n",
            "4      0  #bournemouthbeach oh pandemic people are lying...\n",
            "[2, 0, 0, 2, 0, 0, 0, 0, 2, 0, 2, 2, 0, 0, 0, 2, 0, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 0, 2, 2, 0, 2, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 0, 0, 0, 2, 0, 0, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 2, 2, 0, 0, 2, 2, 2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 2, 0, 2, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0, 0, 2, 2, 0, 0, 2, 0, 0, 2, 0, 2, 0, 2, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 0, 2, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 0, 0, 2, 0, 2, 0, 2, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 0, 2, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 0, 2, 2, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 2, 2, 0, 2, 0, 0, 0, 2, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 0, 0, 2, 2, 2, 0, 0, 0, 0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2, 0, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2, 0, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0, 0, 2, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 0, 2, 0, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 0, 2, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 0, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 0, 0, 2, 2, 0, 0, 0, 2, 2, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 2, 2, 2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2, 0, 0, 0, 2, 2, 0, 0, 2, 2, 0, 0, 2, 2, 0, 0, 2, 2, 0, 0, 0, 2, 0, 2, 0, 2, 2, 2, 2, 0, 0, 2, 0, 0, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0, 2, 2, 2, 0, 0, 0, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 2, 2, 0, 2, 2, 0, 2, 2, 0, 0, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 2, 2, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 2, 0, 2, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 2, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 2, 2, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 0, 0, 2, 2, 0, 0, 0, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0, 0, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 0, 0, 2, 2, 0, 0, 0, 0, 2, 2, 0, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88yxsPsnWQH0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "43808729-54dc-4302-bb00-3e96557ac625"
      },
      "source": [
        "print(len(predictedProbs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFXoBmKWBBBR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "41fb1d0e-861c-4028-9186-b7ba3d304233"
      },
      "source": [
        "\n",
        "testprobs = model2.predict(xemoji)\n",
        "\n",
        "testpreds = np.argmax(testprobs, axis=1)\n",
        "predictedProbs = np.array(sentiments)[testpreds]\n",
        "\n",
        "correct = []\n",
        "for n,i in enumerate(predictedProbs):\n",
        "  if(i == yemoji[n]):\n",
        "    correct.append(i)\n",
        "success = len(correct)/len(predictedProbs)\n",
        "print(success)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl5KF58fxGNi",
        "colab_type": "text"
      },
      "source": [
        "#Pull Tweets and Analyze on the CNN Model since it performed the best."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFSZ9bOtOUmB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2ffdc590-d70a-407c-e58a-1030efc68bc1"
      },
      "source": [
        "import GetOldTweets3 as got\n",
        "import numpy as np\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "base = datetime.datetime(2020, 6, 15)\n",
        "numdays = 30\n",
        "date_list = [base - datetime.timedelta(days=x) for x in range(numdays)]\n",
        "print(date_list)\n",
        "\n",
        "percentageArray = []\n",
        "for date in date_list:\n",
        "    print(date)\n",
        "    tweets = 1000\n",
        "    #Pull tweets from the given date\n",
        "    tweetCriteria = got.manager.TweetCriteria().setQuerySearch('$#BLM-filter:retweets').setSince('2020-05-02').setUntil(date.strftime(\"%Y-%m-%d\")).setMaxTweets(tweets)\n",
        "    tweet = got.manager.TweetManager.getTweets(tweetCriteria)\n",
        "    tweet_list = []\n",
        "    tweet_time = []\n",
        "    for t in tweet:\n",
        "        tweet_list.append(t.text)\n",
        "        tweet_time.append(t.date)\n",
        "    df = pd.DataFrame(tweet_list, tweet_time)\n",
        "    df.to_csv('out.csv')\n",
        "    testdata = pd.read_csv('out.csv')\n",
        "    testdata.columns = ['date', 'tweet']\n",
        "    xtest = unknown*np.ones((len(testdata),MAX_SEQUENCE_LENGTH)).astype(np.int)\n",
        "    sentiments = [0,4,2]\n",
        "\n",
        "\n",
        "\n",
        "    for i,row in testdata.iterrows():\n",
        "        #print(row['text'])\n",
        "        tweet = row['tweet'].split()\n",
        "\n",
        "        ##add space before unicode starter\n",
        "        #xtest[i,:len(tweet)] = [word2index.get(word,unknown) for word in row['tweet'].split()][:MAX_SEQUENCE_LENGTH]\n",
        "        xtest[i,:len(tweet)] = [word2index.get(word,unknown) for word in row['tweet'].split()][:MAX_SEQUENCE_LENGTH]\n",
        "\n",
        "    testprobs = model1.predict(xtest)\n",
        "\n",
        "    testpreds = np.argmax(testprobs, axis=1)\n",
        "    \n",
        "    posnegresults = np.array(sentiments)[testpreds]\n",
        "    pos = np.where(posnegresults == 4)[0]\n",
        "    neg = np.where(posnegresults == 0)[0]\n",
        "    percentagepos = len(pos)/(len(pos)+len(neg))\n",
        "    percentageArray.append(percentagepos)\n",
        "    print(percentagepos)\n",
        "    #Timer to allow continuous pull from Twitter API\n",
        "    time.sleep(60)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[datetime.datetime(2020, 6, 15, 0, 0), datetime.datetime(2020, 6, 14, 0, 0), datetime.datetime(2020, 6, 13, 0, 0), datetime.datetime(2020, 6, 12, 0, 0), datetime.datetime(2020, 6, 11, 0, 0), datetime.datetime(2020, 6, 10, 0, 0), datetime.datetime(2020, 6, 9, 0, 0), datetime.datetime(2020, 6, 8, 0, 0), datetime.datetime(2020, 6, 7, 0, 0), datetime.datetime(2020, 6, 6, 0, 0), datetime.datetime(2020, 6, 5, 0, 0), datetime.datetime(2020, 6, 4, 0, 0), datetime.datetime(2020, 6, 3, 0, 0), datetime.datetime(2020, 6, 2, 0, 0), datetime.datetime(2020, 6, 1, 0, 0), datetime.datetime(2020, 5, 31, 0, 0), datetime.datetime(2020, 5, 30, 0, 0), datetime.datetime(2020, 5, 29, 0, 0), datetime.datetime(2020, 5, 28, 0, 0), datetime.datetime(2020, 5, 27, 0, 0), datetime.datetime(2020, 5, 26, 0, 0), datetime.datetime(2020, 5, 25, 0, 0), datetime.datetime(2020, 5, 24, 0, 0), datetime.datetime(2020, 5, 23, 0, 0), datetime.datetime(2020, 5, 22, 0, 0), datetime.datetime(2020, 5, 21, 0, 0), datetime.datetime(2020, 5, 20, 0, 0), datetime.datetime(2020, 5, 19, 0, 0), datetime.datetime(2020, 5, 18, 0, 0), datetime.datetime(2020, 5, 17, 0, 0)]\n",
            "2020-06-15 00:00:00\n",
            "0.828365878725591\n",
            "2020-06-14 00:00:00\n",
            "0.7917948717948718\n",
            "2020-06-13 00:00:00\n",
            "0.829938900203666\n",
            "2020-06-12 00:00:00\n",
            "0.8101522842639594\n",
            "2020-06-11 00:00:00\n",
            "0.8276923076923077\n",
            "2020-06-10 00:00:00\n",
            "0.8259979529170931\n",
            "2020-06-09 00:00:00\n",
            "0.8587628865979381\n",
            "2020-06-08 00:00:00\n",
            "0.8408624229979466\n",
            "2020-06-07 00:00:00\n",
            "0.8304033092037229\n",
            "2020-06-06 00:00:00\n",
            "0.8591983556012333\n",
            "2020-06-05 00:00:00\n",
            "0.8359293873312564\n",
            "2020-06-04 00:00:00\n",
            "0.8237747653806048\n",
            "2020-06-03 00:00:00\n",
            "0.8224974200206399\n",
            "2020-06-02 00:00:00\n",
            "0.8012486992715921\n",
            "2020-06-01 00:00:00\n",
            "0.7793964620187305\n",
            "2020-05-31 00:00:00\n",
            "0.7767295597484277\n",
            "2020-05-30 00:00:00\n",
            "0.7754459601259182\n",
            "2020-05-29 00:00:00\n",
            "0.7767393561786086\n",
            "2020-05-28 00:00:00\n",
            "0.7739221871713985\n",
            "2020-05-27 00:00:00\n",
            "0.7157360406091371\n",
            "2020-05-26 00:00:00\n",
            "0.8877654196157735\n",
            "2020-05-25 00:00:00\n",
            "0.8748738647830474\n",
            "2020-05-24 00:00:00\n",
            "0.8676767676767677\n",
            "2020-05-23 00:00:00\n",
            "0.8533872598584429\n",
            "2020-05-22 00:00:00\n",
            "0.8135764944275583\n",
            "2020-05-21 00:00:00\n",
            "0.8202020202020202\n",
            "2020-05-20 00:00:00\n",
            "0.8099089989888777\n",
            "2020-05-19 00:00:00\n",
            "0.7922998986828774\n",
            "2020-05-18 00:00:00\n",
            "0.7745197168857432\n",
            "2020-05-17 00:00:00\n",
            "0.7725888324873097\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}